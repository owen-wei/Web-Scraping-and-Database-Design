{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linkedin \n",
    "#find the jobs \"Data Analyst\" in \"California\" with type \"full-time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af8aafc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b16996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome(executable_path = '/Users/oscarhui/Desktop/422/HW4/chromedriver')\n",
    "# driver.implicitly_wait(5)\n",
    "# driver.set_script_timeout(5)\n",
    "# driver.set_page_load_timeout(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264b85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cf102621",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1\n",
    "num_of_pages = 8\n",
    "\n",
    "for i in range(num_of_pages):\n",
    "    URL = f\"https://www.linkedin.com/jobs/search/?currentJobId=3502778692&distance=200.0&geoId=102095887&keywords=Data%20Analyst&start={start}\"\n",
    "    start += 25\n",
    "    Session_requests = requests.Session()\n",
    "    user_agent = {'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36 Edg/88.0.705.56'} \n",
    "\n",
    "    page = Session_requests.get(URL,headers = user_agent)\n",
    "    # print(page.content)\n",
    "    cookies = Session_requests.cookies.get_dict()\n",
    "    # print(cookies)\n",
    "\n",
    "    full_page = BeautifulSoup(page.content,'html.parser')\n",
    "    file = open(f'linkedin_data_analyst_position{i+1}.htm',\"w\")\n",
    "    # #Write the content into the html file\n",
    "    file.write(str(full_page))\n",
    "    time.sleep(3)\n",
    "\n",
    "    # #Close the file\n",
    "    # file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bce0ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_of_pages):\n",
    "    with open('linkedin_data_analyst_position{}.htm'.format(i+1),'r', encoding='utf-8') as f:\n",
    "        page = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(page,'html.parser')\n",
    "    jobs = soup.find_all('a', class_ = \"base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]\")\n",
    "\n",
    "    job_link_list = []\n",
    "    j = 25*i \n",
    "    for job in jobs:\n",
    "        job_link = job['href']\n",
    "        job_link_list.append(job_link)\n",
    "        job_page = requests.get(job_link, headers=user_agent,cookies = cookies)\n",
    "        job_web = BeautifulSoup(job_page.content, \"html.parser\")\n",
    "        with open(f'job{(j+1):03d}.htm','w')as f:\n",
    "            f.write(str(job_web))\n",
    "        time.sleep(3)\n",
    "                \n",
    "        j+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1b33f",
   "metadata": {},
   "source": [
    "from each job page get the info  \n",
    "1.position name (position_name)\n",
    "2.company name (company_name)\n",
    "3.location (location)\n",
    "4.post date (post_date)\n",
    "5.about us (about_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aedf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_name = []\n",
    "company_name = []\n",
    "location = []\n",
    "post_date = []\n",
    "required_skils = []\n",
    "\n",
    "\n",
    "\n",
    "for j in range(0,200):\n",
    "    with open('job{:03d}.htm'.format(j+1),'r', encoding='utf-8') as f:\n",
    "        job_detail_page = f.read()\n",
    "    \n",
    "    position_details_soup = BeautifulSoup(job_detail_page,'html.parser')\n",
    "\n",
    "    p_name = position_details_soup.find_all('h1', class_ = \"top-card-layout__title font-sans text-lg papabear:text-xl font-bold leading-open text-color-text mb-0 topcard__title\")\n",
    "    if p_name:\n",
    "        p_name = p_name[0].text.strip()\n",
    "    else:\n",
    "        p_name = \"N/A\"  \n",
    "    position_name.append(p_name)\n",
    "    \n",
    "    c_name = position_details_soup.find_all('span', class_ = \"topcard__flavor\")\n",
    "    if c_name:\n",
    "        c_name = c_name[0].text.strip()\n",
    "    else:\n",
    "        c_name = \"N/A\"  \n",
    "    company_name.append(c_name)\n",
    "\n",
    "    loca = position_details_soup.find('span', class_ = \"topcard__flavor topcard__flavor--bullet\")\n",
    "    if loca:\n",
    "        loca = loca.text.strip()\n",
    "    else:\n",
    "        loca = \"N/A\"\n",
    "    location.append(loca)\n",
    "    postdate = position_details_soup.find('span', class_ = \"posted-time-ago__text topcard__flavor--metadata\")\n",
    "    if postdate:\n",
    "        postdate = postdate.text.strip()\n",
    "    else:\n",
    "        postdate = \"N/A\"\n",
    "    post_date.append(postdate)\n",
    "    about = position_details_soup.find('div', class_ = \"show-more-less-html__markup\").text\n",
    "    required_skils.append(about)\n",
    "print(position_name)\n",
    "print(company_name)\n",
    "print(location)\n",
    "print(post_date)\n",
    "# print(required_skils)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "511bfa47",
   "metadata": {},
   "source": [
    "### company information\n",
    "#### column name ---- stored list\n",
    "\n",
    "\n",
    "company links --- company_linklist      \n",
    "about us --- aboutus_list\n",
    "company external website link --- web_linklist      \n",
    "number of employees on linkedin --- emp_onlinkedin_numlist     \n",
    "headquarters --- headquarters_list\n",
    "organization type --- org_type_list\n",
    "founded year list --- founded_yr_list\n",
    "specialties --- specialty_list\n",
    "primary location --- main_locationlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "551917ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the companies' links\n",
    "company_linklist = []\n",
    "\n",
    "for j in range(0,200):\n",
    "    with open('job{:03d}.htm'.format(j+1),'r', encoding='utf-8') as f:\n",
    "        job_detail_page = f.read()\n",
    "    \n",
    "    position_details_soup = BeautifulSoup(job_detail_page,'html.parser')\n",
    "    company = position_details_soup.find('a', class_ = \"topcard__org-name-link topcard__flavor--black-link\")\n",
    "    # for company in companies:\n",
    "    # print(company)\n",
    "    company_link = company['href']\n",
    "    #     company_linklist.append(company['href'])\n",
    "#     company_link = company.get('href')\n",
    "    company_linklist.append(company_link)\n",
    "\n",
    "# print(company_linklist)\n",
    "print(len(company_linklist))\n",
    "# # company_linklist\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = {'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36 Edg/88.0.705.56'} \n",
    "\n",
    "for j in range(0,200):\n",
    "    company_page = requests.get(company_linklist[j], headers=user_agent)\n",
    "    # company_web = BeautifulSoup(job_page.content, \"html.parser\")\n",
    "    # print(company_page.content)\n",
    "    company_web = BeautifulSoup(company_page.content, \"html.parser\")\n",
    "    print(company_web)\n",
    "    with open(f'job{(j+1):03d}.htm','w')as f:\n",
    "            f.write(str(company_web))\n",
    "    # with open(f'company{(j+1):03d}.htm','wb')as f:\n",
    "    #         f.write(company_page.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b0db7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "### scrape the page of companies\n",
    "\n",
    "for i in range(0,200):\n",
    "# Set up the Chrome driver\n",
    "        # driver = webdriver.Chrome()\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "        chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "# Navigate to the URL\n",
    "        url = company_linklist[i]\n",
    "        driver.get(url) \n",
    "\n",
    "        # Click on the sign-in button\n",
    "        wait = WebDriverWait(driver, 5)\n",
    "        sign_in_button = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'contextual-sign-in-modal__modal-dismiss-icon.lazy-loaded')))\n",
    "        # sign_in_button = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'full-lockout__action.grow.m-2.max-w-lg.btn-md.btn-primary')))\n",
    "\n",
    "        # sign_in_button = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'sign-in-modal')))\n",
    "# full-lockout__action grow m-2 max-w-lg btn-md btn-primary\n",
    "        sign_in_button.click()\n",
    "        time.sleep(15)\n",
    "\n",
    "        # # Enter the email address\n",
    "        # email_field = wait.until(EC.presence_of_element_located((By.ID, 'organization_guest_contextual-sign-in_sign-in-modal_session_key')))\n",
    "        # email_field.send_keys('oscarhui9525@gmail.com')\n",
    "        # email_field.send_keys(Keys.RETURN)\n",
    "        # time.sleep(5)\n",
    "\n",
    "        # # Enter the password\n",
    "        # password_field = wait.until(EC.presence_of_element_located((By.ID, 'organization_guest_contextual-sign-in_sign-in-modal_session_password')))\n",
    "        # password_field.send_keys('JornJimDavid4x2')\n",
    "        # password_field.send_keys(Keys.RETURN)\n",
    "        \n",
    "        # time.sleep(50)\n",
    "        # # sign_in_button = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'sign-in-modal')))\n",
    "        # # sign_in_button.click()\n",
    "        # # Write the page source to a file\n",
    "        # # element_present = EC.presence_of_element_located((By.ID, 'ember27'))\n",
    "        # # wait.until(element_present)\n",
    "\n",
    "        with open(f'company_page/company{(i+1):03d}.htm', 'w') as f:\n",
    "            f.write(driver.page_source)\n",
    "            i =i+1\n",
    "        time.sleep(5)\n",
    "        # driver.quit()\n",
    "\n",
    "        # driver.quit()\n",
    "# time.sleep(10)\n",
    "# # Close the browser\n",
    "# driver.quit()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "a2b8f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aboutus_list = []\n",
    "web_linklist = []\n",
    "emp_onlinkedin_numlist = []\n",
    "industry_list = []\n",
    "company_sizelist = []\n",
    "headquarters_list = []\n",
    "org_type_list = []\n",
    "founded_yr_list =[]\n",
    "specialty_list = []\n",
    "main_locationlist = []\n",
    "\n",
    "for j in range(0,200):\n",
    "    if j == 99: # There is no info for company 100 so there is no file downloaded\n",
    "       aboutus_list.append('None')\n",
    "       web_linklist.append('None')\n",
    "       industry_list.append('None')\n",
    "       company_sizelist.append('None')\n",
    "       headquarters_list.append('None')\n",
    "       emp_onlinkedin_numlist.append('Not mentioned in Linkedin')\n",
    "       main_locationlist.append('None')\n",
    "       specialty_list.append('None')\n",
    "       founded_yr_list.append('None')\n",
    "       org_type_list.append('None')\n",
    "\n",
    "\n",
    "       continue\n",
    "\n",
    "    with open('company_page/company{:03d}.htm'.format(j+1),'r', encoding='utf-8') as f:\n",
    "        company_detail_page = f.read()\n",
    "    company_details_soup = BeautifulSoup(company_detail_page,'html.parser')\n",
    "    # print(company_details_soup)\n",
    "\n",
    "    # about us\n",
    "    about_us = company_details_soup.find('p', {'data-test-id': 'about-us__description'})\n",
    "    if about_us:\n",
    "        about_us = company_details_soup.find('p', {'data-test-id': 'about-us__description'}).text.strip()\n",
    "        aboutus_list.append(about_us)\n",
    "    else: \n",
    "        aboutus_list.append('None')\n",
    "\n",
    "    # company website link\n",
    "    web_link = company_details_soup.find('a', {'data-tracking-control-name':'about_website'})\n",
    "    if web_link:\n",
    "        web_link = company_details_soup.find('a', {'data-tracking-control-name':'about_website'})['href']\n",
    "        web_linklist.append(web_link)\n",
    "    else:\n",
    "        web_linklist.append('None')\n",
    "    # print(web_link)\n",
    "\n",
    "    # industry\n",
    "    industry = company_details_soup.find('div', {'data-test-id': 'about-us__industries'})\n",
    "    # strip and replace 'Industries' and \\n from results\n",
    "    if industry:\n",
    "        industry = company_details_soup.find('div', {'data-test-id': 'about-us__industries'}).text.strip()\n",
    "        industry=' '.join(industry.split()[-5:]).strip()  \n",
    "        industry = industry.replace('Industries ','')\n",
    "        industry_list.append(industry)\n",
    "    else:\n",
    "        industry_list.append('None')\n",
    "  \n",
    "    # company size\n",
    "    company_size = company_details_soup.find('div', {'data-test-id': 'about-us__size'})\n",
    "    if company_size:\n",
    "        company_size = company_details_soup.find('div', {'data-test-id': 'about-us__size'}).text.strip()\n",
    "        company_size=' '.join(company_size.split()[-2:]).strip()  \n",
    "        company_size = company_size.replace('Company size ','')\n",
    "        company_sizelist.append(company_size)\n",
    "    else:\n",
    "        company_sizelist.append('None')\n",
    "    # print(company_size)\n",
    "\n",
    "    # headquarters\n",
    "    headquarters = company_details_soup.find('div', {'data-test-id': 'about-us__headquarters'})\n",
    "    if headquarters:\n",
    "        headquarters = company_details_soup.find('div', {'data-test-id': 'about-us__headquarters'}).text.strip()\n",
    "        headquarters=' '.join(headquarters.split()[-4:]).strip()  \n",
    "        headquarters = headquarters.replace('Headquarters ','')\n",
    "        headquarters_list.append(headquarters)\n",
    "    else:\n",
    "        headquarters_list.append('None')\n",
    "  \n",
    "\n",
    "    # organization type\n",
    "    org_type = company_details_soup.find('div', {'data-test-id': 'about-us__organizationType'})\n",
    "    if org_type:\n",
    "        org_type = company_details_soup.find('div', {'data-test-id': 'about-us__organizationType'}).text.strip()\n",
    "        org_type=' '.join(org_type.split()[-4:]).strip()  \n",
    "        org_type = org_type.replace('Type ','')\n",
    "        org_type_list.append(org_type)\n",
    "    else:\n",
    "        org_type_list.append('None')\n",
    "\n",
    "\n",
    "    # founded year\n",
    "    founded_yr = company_details_soup.find('div', {'data-test-id': 'about-us__foundedOn'})\n",
    "    if founded_yr:\n",
    "        founded_yr = company_details_soup.find('div', {'data-test-id': 'about-us__foundedOn'}).text.strip()\n",
    "        founded_yr=' '.join(founded_yr.split()[-4:]).strip()  \n",
    "        founded_yr = founded_yr.replace('Founded ','')\n",
    "        founded_yr_list.append(founded_yr)\n",
    "    else:\n",
    "        founded_yr_list.append('None')\n",
    "\n",
    "    # specialty\n",
    "    specialty = company_details_soup.find('div', {'data-test-id': 'about-us__specialties'})\n",
    "    if specialty:\n",
    "        specialty = company_details_soup.find('div', {'data-test-id': 'about-us__specialties'}).text.strip()\n",
    "        specialty=' '.join(specialty.split()[-100:]).strip()  \n",
    "        specialty = specialty.replace('Specialties ','')\n",
    "        specialty_list.append(specialty)\n",
    "    else:\n",
    "        specialty_list.append('None')\n",
    "    \n",
    "    # primary location\n",
    "    main_location = company_details_soup.find('div', id = \"address-0\")\n",
    "    if main_location:\n",
    "        main_location = company_details_soup.find('div', id = \"address-0\").text.strip()\n",
    "        main_location=' '.join(main_location.split()[-100:]).strip()  \n",
    "        main_locationlist.append(main_location)\n",
    "    else:\n",
    "        main_locationlist.append('None')\n",
    "    \n",
    "    # number of employees on linkedin\n",
    "    emp_num_info = company_details_soup.find('a', class_ = \"face-pile__cta self-center link-no-visited-state\")\n",
    "    try: \n",
    "        emp_num_info = company_details_soup.find('a', class_ = \"face-pile__cta self-center link-no-visited-state\").text.strip() \n",
    "        pattern = r'\\d{1,3}(?:,\\d{3})*'  # regular expression to match numbers with commas ?: for non capturing group\n",
    "\n",
    "        result = re.findall(pattern, emp_num_info)\n",
    "        if result:\n",
    "            emp_num = result[0].replace(',', '')  # remove commas from the matched number\n",
    "            emp_onlinkedin_numlist.append(emp_num)\n",
    "        else:\n",
    "            emp_onlinkedin_numlist.append('Not mentioned in Linkedin')\n",
    "    except:\n",
    "            emp_onlinkedin_numlist.append('Not mentioned in Linkedin')\n",
    "    # print(emp_num)\n",
    "    j += j\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "475846f3",
   "metadata": {},
   "source": [
    "### put into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7758bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "ed07a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "\n",
    "\n",
    "# Create a new database and collection\n",
    "# Initialize the 'rarity' dictionary with None value\n",
    "db = client['DDR_final_project']\n",
    "collection = db['Linkedin_job_info']\n",
    "collection.delete_many({}) # make sure every time when I run this function, the collection is NA\n",
    "\n",
    "\n",
    "for j in range(len(position_name)):\n",
    "        job_data = {\n",
    "        'position_name': None, \n",
    "        'job_location': None,\n",
    "        'job_post_data': None,\n",
    "        'job_required_skills': None,\n",
    "        'company_info': []\n",
    "    }\n",
    "        job_data['position_name'] = position_name[j]\n",
    "        job_data['job_location'] = location[j]\n",
    "        job_data['job_post_data'] = post_date[j]\n",
    "        job_data['job_required_skills'] = required_skils[j]\n",
    "        job_data['company_info'] = []\n",
    "        company_data = {\n",
    "            'company_name': company_name[j],\n",
    "            'company_description': aboutus_list[j],\n",
    "            'company_external_link': web_linklist[j],\n",
    "            'number_of_employees_on_linkedin': emp_onlinkedin_numlist[j],\n",
    "            'industry': industry_list[j],\n",
    "            'company_size': company_sizelist[j],\n",
    "            'headquarters': headquarters_list[j],\n",
    "            'organization_type': org_type_list[j],\n",
    "            'founded_year': founded_yr_list[j],\n",
    "            'specialties': specialty_list[j],\n",
    "            'main_location': main_locationlist[j]\n",
    "        }\n",
    "        job_data['company_info'].append(company_data)\n",
    "        j=j+1\n",
    "\n",
    "        collection.insert_one(job_data)\n",
    "        # print(list(collection.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e29e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b5c248580b0fbe3065afc11d2c48905023f666036e44592783621b87235b9f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
